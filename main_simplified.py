#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon Japan å–å®¶ä¿¡æ¯æå–å·¥å…· - ç®€åŒ–é«˜æ€§èƒ½ç‰ˆæœ¬
Amazon Japan Seller Information Extractor - Simplified High-Performance Version
"""

import sys
import os
import time
import random
import requests
from datetime import datetime
from urllib.parse import urljoin, quote
import pandas as pd
from bs4 import BeautifulSoup
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import threading
from queue import Queue, Empty
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class SimplifiedAmazonScraper:
    """ç®€åŒ–çš„Amazonçˆ¬è™«ç±» - ä¸“æ³¨äºå…³é”®è¯æœç´¢"""
    
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://www.amazon.co.jp"
        
        # é…ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
        
        # é…ç½®è¿æ¥æ± å’Œé‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(
            pool_connections=5,
            pool_maxsize=10,
            max_retries=retry_strategy
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # æ€§èƒ½é…ç½®
        self.max_concurrent_requests = 3  # é™ä½å¹¶å‘æ•°ï¼Œæé«˜ç¨³å®šæ€§
        self.request_delay_range = (1.0, 2.0)  # é€‚ä¸­çš„å»¶è¿Ÿ
        self.batch_size = 20  # åˆ†æ‰¹å¤„ç†å¤§å°
        
    def search_products(self, keyword, max_pages=20, max_products=1000, progress_callback=None, stop_flag=None):
        """
        ç®€åŒ–çš„äº§å“æœç´¢æ–¹æ³•
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_pages: æœ€å¤§é¡µæ•°
            max_products: æœ€å¤§äº§å“æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            stop_flag: åœæ­¢æ ‡å¿—å‡½æ•°
        
        Returns:
            list: äº§å“ä¿¡æ¯åˆ—è¡¨
        """
        products = []
        page = 1
        
        try:
            while page <= max_pages and len(products) < max_products:
                if stop_flag and not stop_flag():
                    break
                
                # æ„å»ºæœç´¢URL
                search_params = {
                    'k': keyword,
                    'page': page,
                    'ref': 'sr_pg_' + str(page)
                }
                
                search_url = f"{self.base_url}/s"
                
                if progress_callback:
                    progress_callback(f"æ­£åœ¨æœç´¢ç¬¬ {page} é¡µ...")
                
                try:
                    response = self.session.get(search_url, params=search_params, timeout=15)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æŸ¥æ‰¾äº§å“å…ƒç´ 
                    product_selectors = [
                        'div[data-component-type="s-search-result"]',
                        '.s-result-item[data-component-type="s-search-result"]',
                        '.s-search-result'
                    ]
                    
                    product_items = []
                    for selector in product_selectors:
                        product_items = soup.select(selector)
                        if product_items:
                            break
                    
                    if not product_items:
                        print(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°äº§å“ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                        break
                    
                    page_products = 0
                    for item in product_items:
                        if len(products) >= max_products:
                            break
                            
                        product_info = self._extract_product_info(item)
                        if product_info:
                            products.append(product_info)
                            page_products += 1
                    
                    if progress_callback:
                        progress_callback(f"ç¬¬ {page} é¡µæ‰¾åˆ° {page_products} ä¸ªäº§å“ï¼Œæ€»è®¡ {len(products)} ä¸ª")
                    
                    if page_products == 0:
                        print("è¿ç»­é¡µé¢æ— äº§å“ï¼Œåœæ­¢æœç´¢")
                        break
                    
                    page += 1
                    
                    # éšæœºå»¶è¿Ÿ
                    delay = random.uniform(*self.request_delay_range)
                    time.sleep(delay)
                    
                except requests.RequestException as e:
                    print(f"æœç´¢ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
                    page += 1
                    continue
                    
        except Exception as e:
            print(f"æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
        
        return products
    
    def _extract_product_info(self, product_element):
        """ä»äº§å“å…ƒç´ ä¸­æå–åŸºæœ¬ä¿¡æ¯"""
        try:
            if not product_element:
                return None
            
            # äº§å“é“¾æ¥
            link_element = (product_element.select_one('h2 a') or
                          product_element.select_one('.a-link-normal') or
                          product_element.select_one('a[href*="/dp/"]'))
            
            if not link_element:
                return None
            
            product_url = urljoin(self.base_url, link_element.get('href', ''))
            if not product_url or '/dp/' not in product_url:
                return None
            
            # äº§å“æ ‡é¢˜
            title_element = (link_element.select_one('span') or
                           product_element.select_one('.a-size-mini span') or
                           product_element.select_one('.a-size-base-plus'))
            
            title = title_element.get_text(strip=True) if title_element else "æœªçŸ¥äº§å“"
            
            # ä»·æ ¼ä¿¡æ¯
            price_element = (product_element.select_one('.a-price-whole') or
                           product_element.select_one('.a-price .a-offscreen') or
                           product_element.select_one('.a-price-range'))
            
            price = price_element.get_text(strip=True) if price_element else "ä»·æ ¼æœªçŸ¥"
            
            return {
                'title': title[:100],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                'price': price,
                'url': product_url
            }
            
        except Exception as e:
            print(f"æå–äº§å“ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def get_seller_info_batch(self, products, progress_callback=None, stop_flag=None):
        """
        æ‰¹é‡è·å–å–å®¶ä¿¡æ¯ - åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜é—®é¢˜
        
        Args:
            products: äº§å“åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            stop_flag: åœæ­¢æ ‡å¿—å‡½æ•°
        
        Returns:
            list: åŒ…å«å–å®¶ä¿¡æ¯çš„ç»“æœåˆ—è¡¨
        """
        results = []
        total_products = len(products)
        completed = 0
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_products, self.batch_size):
            if stop_flag and not stop_flag():
                break
            
            batch_end = min(batch_start + self.batch_size, total_products)
            batch_products = products[batch_start:batch_end]
            
            if progress_callback:
                progress_callback(f"å¤„ç†æ‰¹æ¬¡ {batch_start//self.batch_size + 1}/{(total_products-1)//self.batch_size + 1}")
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results = self._process_batch(batch_products, progress_callback, stop_flag, completed, total_products)
            results.extend(batch_results)
            completed += len(batch_products)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            time.sleep(1)
        
        return results
    
    def _process_batch(self, batch_products, progress_callback, stop_flag, base_completed, total_products):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„äº§å“"""
        batch_results = []
        
        def fetch_single_seller(product, index):
            try:
                if stop_flag and not stop_flag():
                    return None
                
                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(*self.request_delay_range))
                
                seller_info = self._get_seller_info(product['url'])
                
                result = {
                    'product_title': product['title'],
                    'price': product['price'],
                    'product_url': product['url'],
                    'seller_name': seller_info.get('seller_name', 'æœªçŸ¥') if seller_info else 'æœªçŸ¥',
                    'business_name': seller_info.get('business_name', '') if seller_info else '',
                    'store_name': seller_info.get('store_name', '') if seller_info else '',
                    'phone': seller_info.get('phone', '') if seller_info else '',
                    'address': seller_info.get('address', '') if seller_info else '',
                    'representative': seller_info.get('representative', '') if seller_info else '',
                    'seller_url': seller_info.get('seller_url', '') if seller_info else ''
                }
                
                # æ›´æ–°è¿›åº¦
                current_completed = base_completed + index + 1
                if progress_callback:
                    progress_callback(f"å·²å¤„ç† {current_completed}/{total_products} ä¸ªäº§å“")
                
                return result
                
            except Exception as e:
                print(f"å¤„ç†äº§å“å¤±è´¥ {product.get('title', 'Unknown')}: {e}")
                return {
                    'product_title': product.get('title', 'æœªçŸ¥'),
                    'price': product.get('price', ''),
                    'product_url': product.get('url', ''),
                    'seller_name': 'è·å–å¤±è´¥',
                    'business_name': '', 'store_name': '', 'phone': '', 'address': '', 'representative': '', 'seller_url': ''
                }
        
        # ä½¿ç”¨è¾ƒå°çš„çº¿ç¨‹æ± å¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=self.max_concurrent_requests) as executor:
            future_to_product = {
                executor.submit(fetch_single_seller, product, i): product 
                for i, product in enumerate(batch_products)
            }
            
            for future in as_completed(future_to_product):
                try:
                    if stop_flag and not stop_flag():
                        break
                    
                    result = future.result()
                    if result:
                        batch_results.append(result)
                        
                except Exception as e:
                    print(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {e}")
        
        return batch_results
    
    def _get_seller_info(self, product_url):
        """è·å–å•ä¸ªäº§å“çš„å–å®¶ä¿¡æ¯"""
        try:
            response = self.session.get(product_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å–å®¶ä¿¡æ¯
            seller_info = {}
            
            # å–å®¶åç§°å’Œé“¾æ¥
            seller_element = (soup.find('a', {'id': 'sellerProfileTriggerId'}) or
                            soup.find('a', string=re.compile(r'.*è²©å£².*')) or
                            soup.select_one('#merchant-info a'))
            
            if seller_element:
                seller_info['seller_name'] = seller_element.get_text(strip=True)
                seller_url = seller_element.get('href', '')
                if seller_url:
                    seller_info['seller_url'] = urljoin(self.base_url, seller_url)
                    
                    # è·å–è¯¦ç»†å–å®¶ä¿¡æ¯
                    detailed_info = self._get_detailed_seller_info(seller_info['seller_url'])
                    if detailed_info:
                        seller_info.update(detailed_info)
            
            return seller_info if seller_info else None
            
        except Exception as e:
            print(f"è·å–å–å®¶ä¿¡æ¯å¤±è´¥ {product_url}: {e}")
            return None
    
    def _get_detailed_seller_info(self, seller_url):
        """è·å–è¯¦ç»†å–å®¶ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬"""
        try:
            response = self.session.get(seller_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ–¹æ³•1: åŸºäºå…³é”®è¯å’Œä¸Šä¸‹æ–‡çš„æ™ºèƒ½æå–
            smart_info = self._smart_extract_seller_info(soup)
            
            # æ–¹æ³•2: åŸºäºHTMLç»“æ„æå–
            structured_info = self._extract_from_html_structure(soup)
            
            # æ–¹æ³•3: ä¼ ç»Ÿæ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆä½œä¸ºåå¤‡ï¼‰
            regex_info = self._extract_with_regex(soup.get_text())
            
            # åˆå¹¶ç»“æœï¼Œä¼˜å…ˆçº§ï¼šæ™ºèƒ½æå– > ç»“æ„æå– > æ­£åˆ™æå–
            final_info = {}
            fields = ['business_name', 'phone', 'address', 'representative', 'store_name']
            
            for field in fields:
                final_info[field] = (
                    smart_info.get(field) or 
                    structured_info.get(field) or 
                    regex_info.get(field) or 
                    ''
                )
            
            # æ¸…ç†å’ŒéªŒè¯ç»“æœ
            final_info = self._clean_seller_info(final_info)
            
            return final_info
            
        except Exception as e:
            print(f"è·å–è¯¦ç»†å–å®¶ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _smart_extract_seller_info(self, soup):
        """åŸºäºå…³é”®è¯å…³è”çš„æ™ºèƒ½æå–"""
        info = {}
        
        # å…³é”®è¯æ˜ å°„è¡¨ - åŸºäºAmazonå®é™…é¡µé¢
        field_keywords = {
            'business_name': [
                'Business Name', 'business name', 'ä¼šç¤¾å', 'å•†å·', 'ä¼ä¸šåç§°', 
                'Company Name', 'company name', 'å…¬å¸åç§°'
            ],
            'phone': [
                'å’¨è¯¢ç”¨ç”µè¯å·ç ', 'ç”µè¯å·ç ', 'é›»è©±ç•ªå·', 'TEL', 'Tel',
                'Phone', 'phone', 'Telephone', 'è”ç³»ç”µè¯', 'å’¨è¯¢ç”µè¯'
            ],
            'address': [
                'åœ°å€', 'ä½æ‰€', 'æ‰€åœ¨åœ°', 'Address', 'address',
                'è”ç³»åœ°å€', 'å…¬å¸åœ°å€', 'è¥ä¸šåœ°å€'
            ],
            'representative': [
                'è´­ç‰©ä»£è¡¨çš„å§“å', 'ä»£è¡¨è€…', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…',
                'Representative', 'representative', 'è”ç³»äºº', 'è´Ÿè´£äºº', 'ä»£è¡¨äºº'
            ],
            'store_name': [
                'å•†åº—å', 'åº—èˆ—å', 'ã‚·ãƒ§ãƒƒãƒ—å', 'Store Name', 'store name',
                'Shop Name', 'shop name', 'åº—é“ºåç§°'
            ]
        }
        
        # æŸ¥æ‰¾åŒ…å«å–å®¶ä¿¡æ¯çš„åŒºåŸŸ
        seller_sections = soup.find_all(['div', 'section', 'table'], 
                                       string=re.compile(r'è¯¦å°½çš„å–å®¶ä¿¡æ¯|è¯¦æ°‘çš„å–å®¶ä¿¡æ¯|seller.*info', re.I))
        
        if not seller_sections:
            # æ‰©å¤§æœç´¢èŒƒå›´
            seller_sections = soup.find_all(['div', 'section', 'table'])
        
        for section in seller_sections[:5]:  # é™åˆ¶æœç´¢èŒƒå›´
            section_text = section.get_text() if section else ""
            
            for field, keywords in field_keywords.items():
                if info.get(field):  # å·²ç»æ‰¾åˆ°è¯¥å­—æ®µ
                    continue
                
                for keyword in keywords:
                    # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å…³é”®è¯
                    if keyword.lower() in section_text.lower():
                        value = self._extract_value_near_keyword(section_text, keyword, field)
                        if value and self._validate_extracted_value(field, value):
                            info[field] = value
                            break
                
                if info.get(field):
                    break
        
        return info
    
    def _extract_from_html_structure(self, soup):
        """åŸºäºHTMLç»“æ„æå–"""
        info = {}
        
        # æŸ¥æ‰¾è¡¨æ ¼ç»“æ„
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key_text = cells[0].get_text().strip()
                    value_text = cells[1].get_text().strip()
                    
                    # åŒ¹é…å­—æ®µ
                    if not info.get('business_name') and any(k in key_text.lower() for k in ['business', 'ä¼šç¤¾', 'å•†å·']):
                        info['business_name'] = value_text
                    elif not info.get('phone') and any(k in key_text.lower() for k in ['ç”µè¯', 'tel', 'phone']):
                        info['phone'] = value_text
                    elif not info.get('address') and any(k in key_text.lower() for k in ['åœ°å€', 'ä½æ‰€', 'address']):
                        info['address'] = value_text
                    elif not info.get('representative') and any(k in key_text.lower() for k in ['ä»£è¡¨', 'representative']):
                        info['representative'] = value_text
                    elif not info.get('store_name') and any(k in key_text.lower() for k in ['åº—', 'store', 'shop']):
                        info['store_name'] = value_text
        
        return info
    
    def _extract_with_regex(self, text):
        """ä¼ ç»Ÿæ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆåå¤‡æ–¹æ³•ï¼‰"""
        info = {}
        
        # å¢å¼ºçš„æ­£åˆ™æ¨¡å¼
        patterns = {
            'business_name': [
                r'Business\s*Name[ï¼š:\s]*([^\n\r]{3,50})',
                r'ä¼šç¤¾å[ï¼š:\s]*([^\n\r]{3,50})',
                r'å•†å·[ï¼š:\s]*([^\n\r]{3,50})',
                r'([A-Za-z\s]{3,30}(?:æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|Company|Ltd|Corporation|Inc))',
            ],
            'phone': [
                r'å’¨è¯¢ç”¨ç”µè¯å·ç [ï¼š:\s]*(\+?[\d\-\(\)\s]{8,20})',
                r'é›»è©±ç•ªå·[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,20})',
                r'TEL[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,20})',
                r'(\+?\d{1,3}[-\s]?\d{10,11})',
                r'(\d{2,4}[-\s]\d{4}[-\s]\d{4})',
            ],
            'address': [
                r'åœ°å€[ï¼š:\s]*([^\n\r]{10,100})',
                r'ä½æ‰€[ï¼š:\s]*([^\n\r]{10,100})',
                r'Address[ï¼š:\s]*([^\n\r]{10,100})',
                r'(ã€’\d{3}-\d{4}[^\n\r]{5,80})',
            ],
            'representative': [
                r'è´­ç‰©ä»£è¡¨çš„å§“å[ï¼š:\s]*([^\n\r]{2,30})',
                r'ä»£è¡¨è€…[ï¼š:\s]*([^\n\r]{2,30})',
                r'ä»£è¡¨å–ç· å½¹[ï¼š:\s]*([^\n\r]{2,30})',
                r'Representative[ï¼š:\s]*([^\n\r]{2,30})',
            ],
            'store_name': [
                r'å•†åº—å[ï¼š:\s]*([^\n\r]{2,30})',
                r'åº—èˆ—å[ï¼š:\s]*([^\n\r]{2,30})',
                r'Store\s*Name[ï¼š:\s]*([^\n\r]{2,30})',
                r'ã‚·ãƒ§ãƒƒãƒ—å[ï¼š:\s]*([^\n\r]{2,30})',
            ]
        }
        
        for field, field_patterns in patterns.items():
            for pattern in field_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    value = match.group(1).strip()
                    if self._validate_extracted_value(field, value):
                        info[field] = value
                        break
            
        return info
    
    def _extract_value_near_keyword(self, text, keyword, field):
        """æå–å…³é”®è¯é™„è¿‘çš„å€¼"""
        try:
            # æ‰¾åˆ°å…³é”®è¯ä½ç½®
            keyword_pos = text.lower().find(keyword.lower())
            if keyword_pos == -1:
                return None
            
            # æå–å…³é”®è¯åçš„æ–‡æœ¬
            after_keyword = text[keyword_pos + len(keyword):].strip()
            
            # ç§»é™¤å¼€å¤´çš„åˆ†éš”ç¬¦
            after_keyword = re.sub(r'^[ï¼š:\s\-]+', '', after_keyword)
            
            if field == 'phone':
                # ç”µè¯å·ç ç‰¹æ®Šå¤„ç†
                phone_patterns = [
                    r'\+?\d{1,3}[-\s]?\d{10,11}',
                    r'\+?\d{11,13}',
                    r'\d{2,4}[-\s]\d{4}[-\s]\d{4}',
                ]
                for pattern in phone_patterns:
                    match = re.search(pattern, after_keyword)
                    if match:
                        return match.group(0)
            elif field == 'address':
                # åœ°å€é€šå¸¸æ˜¯å¤šè¡Œçš„
                lines = after_keyword.split('\n')[:3]  # å–å‰3è¡Œ
                address_parts = []
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 3:
                        address_parts.append(line)
                        if len(' '.join(address_parts)) > 15:  # åœ°å€è¶³å¤Ÿé•¿
                            break
                return ' '.join(address_parts) if address_parts else None
            else:
                # å…¶ä»–å­—æ®µå–ç¬¬ä¸€è¡Œ
                first_line = after_keyword.split('\n')[0].strip()
                # ç§»é™¤å¯èƒ½çš„åç»­å­—æ®µæ ‡è¯†
                first_line = re.split(r'[ï¼š:]', first_line)[0].strip()
                return first_line if len(first_line) > 1 else None
            
        except Exception:
            return None
    
    def _validate_extracted_value(self, field, value):
        """éªŒè¯æå–çš„å€¼æ˜¯å¦åˆç†"""
        if not value or len(value.strip()) < 2:
            return False
        
        value = value.strip()
        
        if field == 'phone':
            # ç”µè¯å·ç å¿…é¡»åŒ…å«è¶³å¤Ÿçš„æ•°å­—
            digit_count = len(re.findall(r'\d', value))
            return digit_count >= 8 and digit_count <= 15
        elif field == 'address':
            # åœ°å€å¿…é¡»æœ‰ä¸€å®šé•¿åº¦ä¸”åŒ…å«æœ‰æ„ä¹‰å­—ç¬¦
            return len(value) >= 8 and bool(re.search(r'[\u4e00-\u9fff]|[a-zA-Z]', value))
        elif field == 'business_name':
            # å…¬å¸åä¸èƒ½å¤ªçŸ­æˆ–å¤ªé•¿
            return 3 <= len(value) <= 80
        elif field == 'representative':
            # ä»£è¡¨äººå§“åé•¿åº¦åˆç†
            return 2 <= len(value) <= 40
        elif field == 'store_name':
            # åº—é“ºåé•¿åº¦åˆç†
            return 2 <= len(value) <= 40
        
        return True
    
    def _clean_seller_info(self, info):
        """æ¸…ç†å–å®¶ä¿¡æ¯"""
        cleaned = {}
        
        for field, value in info.items():
            if not value:
                cleaned[field] = ''
                continue
            
            # åŸºæœ¬æ¸…ç†
            value = str(value).strip()
            value = re.sub(r'\s+', ' ', value)  # åˆå¹¶ç©ºæ ¼
            value = re.sub(r'^[ï¼š:\-\s]+', '', value)  # ç§»é™¤å¼€å¤´åˆ†éš”ç¬¦
            value = re.sub(r'[ï¼š:\-\s]+$', '', value)  # ç§»é™¤ç»“å°¾åˆ†éš”ç¬¦
            
            # å­—æ®µç‰¹å®šæ¸…ç†
            if field == 'phone':
                # ä¿ç•™æ•°å­—ã€åŠ å·ã€å‡å·ã€æ‹¬å·ã€ç©ºæ ¼
                value = re.sub(r'[^\d\+\-\(\)\s]', '', value)
                value = re.sub(r'\s+', '', value)  # ç§»é™¤ç©ºæ ¼ä½¿æ ¼å¼ç»Ÿä¸€
            elif field == 'address':
                # åœ°å€æ¸…ç†æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
                value = re.sub(r'\n+', ' ', value)
                value = re.sub(r'\s{2,}', ' ', value)
            
            # é™åˆ¶é•¿åº¦
            if field == 'address':
                cleaned[field] = value[:150]
            else:
                cleaned[field] = value[:100]
        
        return cleaned


class SimplifiedScraperGUI:
    """ç®€åŒ–çš„GUIç•Œé¢"""
    
    def __init__(self, root):
        self.root = root
        self.root.title("Amazon Japan å–å®¶ä¿¡æ¯æå–å·¥å…· v3.0 - ç®€åŒ–é«˜æ€§èƒ½ç‰ˆ")
        self.root.geometry("1000x700")
        self.root.resizable(True, True)
        
        # åˆå§‹åŒ–çˆ¬è™«
        self.scraper = SimplifiedAmazonScraper()
        
        # çŠ¶æ€å˜é‡
        self.is_scraping = False
        self.scraping_thread = None
        self.results = []
        
        # åˆ›å»ºç•Œé¢
        self.setup_ui()
        
    def setup_ui(self):
        """è®¾ç½®ç”¨æˆ·ç•Œé¢"""
        # ä¸»æ¡†æ¶
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # é…ç½®æƒé‡
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(3, weight=1)
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text="ğŸ›’ Amazon Japan å–å®¶ä¿¡æ¯æå–å·¥å…·", 
                               font=('Arial', 16, 'bold'))
        title_label.grid(row=0, column=0, pady=(0, 20))
        
        # æœç´¢é…ç½®æ¡†æ¶
        config_frame = ttk.LabelFrame(main_frame, text="æœç´¢é…ç½®", padding="10")
        config_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        config_frame.columnconfigure(1, weight=1)
        
        # å…³é”®è¯è¾“å…¥
        ttk.Label(config_frame, text="æœç´¢å…³é”®è¯:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.keyword_var = tk.StringVar(value="ç”µè„‘")
        keyword_entry = ttk.Entry(config_frame, textvariable=self.keyword_var, width=30)
        keyword_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), pady=5, padx=(10, 0))
        
        # å‚æ•°è®¾ç½®
        params_frame = ttk.Frame(config_frame)
        params_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=10)
        
        ttk.Label(params_frame, text="æœ€å¤§é¡µæ•°:").grid(row=0, column=0, sticky=tk.W)
        self.pages_var = tk.StringVar(value="20")
        pages_spinbox = ttk.Spinbox(params_frame, from_=1, to=100, textvariable=self.pages_var, width=10)
        pages_spinbox.grid(row=0, column=1, padx=(5, 20))
        
        ttk.Label(params_frame, text="æœ€å¤§äº§å“æ•°:").grid(row=0, column=2, sticky=tk.W)
        self.max_products_var = tk.StringVar(value="1000")
        products_spinbox = ttk.Spinbox(params_frame, from_=50, to=10000, increment=50, 
                                     textvariable=self.max_products_var, width=10)
        products_spinbox.grid(row=0, column=3, padx=(5, 20))
        
        ttk.Label(params_frame, text="å¹¶å‘æ•°:").grid(row=0, column=4, sticky=tk.W)
        self.concurrent_var = tk.StringVar(value="3")
        concurrent_spinbox = ttk.Spinbox(params_frame, from_=1, to=5, textvariable=self.concurrent_var, width=10)
        concurrent_spinbox.grid(row=0, column=5, padx=(5, 0))
        
        # æ§åˆ¶æŒ‰é’®
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=2, column=0, pady=10)
        
        self.start_button = ttk.Button(button_frame, text="ğŸš€ å¼€å§‹æœç´¢", command=self.start_scraping)
        self.start_button.pack(side=tk.LEFT, padx=5)
        
        self.stop_button = ttk.Button(button_frame, text="â¹ï¸ åœæ­¢", command=self.stop_scraping, state=tk.DISABLED)
        self.stop_button.pack(side=tk.LEFT, padx=5)
        
        self.export_button = ttk.Button(button_frame, text="ğŸ“Š å¯¼å‡ºæ•°æ®", command=self.export_data, state=tk.DISABLED)
        self.export_button.pack(side=tk.LEFT, padx=5)
        
        self.clear_button = ttk.Button(button_frame, text="ğŸ—‘ï¸ æ¸…ç©º", command=self.clear_results)
        self.clear_button.pack(side=tk.LEFT, padx=5)
        
        # è¿›åº¦å’ŒçŠ¶æ€
        status_frame = ttk.Frame(main_frame)
        status_frame.grid(row=3, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        status_frame.columnconfigure(0, weight=1)
        
        self.status_var = tk.StringVar(value="å°±ç»ª")
        status_label = ttk.Label(status_frame, textvariable=self.status_var)
        status_label.grid(row=0, column=0, sticky=tk.W)
        
        self.progress_bar = ttk.Progressbar(status_frame, mode='indeterminate')
        self.progress_bar.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=5)
        
        # ç»“æœè¡¨æ ¼
        results_frame = ttk.LabelFrame(main_frame, text="æå–ç»“æœ", padding="5")
        results_frame.grid(row=4, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(10, 0))
        results_frame.columnconfigure(0, weight=1)
        results_frame.rowconfigure(0, weight=1)
        
        # åˆ›å»ºè¡¨æ ¼
        columns = ('äº§å“åç§°', 'ä»·æ ¼', 'å–å®¶åç§°', 'å•†å®¶åç§°', 'åº—é“ºåç§°', 'ç”µè¯', 'åœ°å€', 'ä»£è¡¨äºº')
        self.tree = ttk.Treeview(results_frame, columns=columns, show='headings', height=15)
        
        # è®¾ç½®åˆ—æ ‡é¢˜å’Œå®½åº¦
        column_widths = [200, 80, 100, 120, 100, 100, 150, 80]
        for col, width in zip(columns, column_widths):
            self.tree.heading(col, text=col)
            self.tree.column(col, width=width, minwidth=50)
        
        # æ»šåŠ¨æ¡
        scrollbar_y = ttk.Scrollbar(results_frame, orient=tk.VERTICAL, command=self.tree.yview)
        scrollbar_x = ttk.Scrollbar(results_frame, orient=tk.HORIZONTAL, command=self.tree.xview)
        self.tree.configure(yscrollcommand=scrollbar_y.set, xscrollcommand=scrollbar_x.set)
        
        # å¸ƒå±€
        self.tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar_y.grid(row=0, column=1, sticky=(tk.N, tk.S))
        scrollbar_x.grid(row=1, column=0, sticky=(tk.W, tk.E))
    
    def start_scraping(self):
        """å¼€å§‹çˆ¬å–"""
        if self.is_scraping:
            return
        
        keyword = self.keyword_var.get().strip()
        if not keyword:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥æœç´¢å…³é”®è¯")
            return
        
        try:
            pages = int(self.pages_var.get())
            max_products = int(self.max_products_var.get())
            concurrent = int(self.concurrent_var.get())
        except ValueError:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å€¼")
            return
        
        # æ›´æ–°çˆ¬è™«é…ç½®
        self.scraper.max_concurrent_requests = concurrent
        
        # æ¸…ç©ºç»“æœ
        self.clear_results()
        
        # æ›´æ–°UIçŠ¶æ€
        self.is_scraping = True
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.export_button.config(state=tk.DISABLED)
        self.progress_bar.start()
        
        # å¯åŠ¨çˆ¬è™«çº¿ç¨‹
        self.scraping_thread = threading.Thread(
            target=self.scraping_worker,
            args=(keyword, pages, max_products)
        )
        self.scraping_thread.daemon = True
        self.scraping_thread.start()
    
    def stop_scraping(self):
        """åœæ­¢çˆ¬å–"""
        self.is_scraping = False
        self.update_status("æ­£åœ¨åœæ­¢...")
    
    def scraping_worker(self, keyword, pages, max_products):
        """çˆ¬è™«å·¥ä½œçº¿ç¨‹"""
        try:
            # æœç´¢äº§å“
            self.update_status(f"æ­£åœ¨æœç´¢å…³é”®è¯: {keyword}")
            
            def search_progress(message):
                self.update_status(message)
            
            def stop_flag():
                return self.is_scraping
            
            products = self.scraper.search_products(
                keyword, pages, max_products, 
                progress_callback=search_progress,
                stop_flag=stop_flag
            )
            
            if not products:
                self.update_status("æœªæ‰¾åˆ°ä»»ä½•äº§å“")
                self.scraping_finished(0, 0)
                return
            
            self.update_status(f"æ‰¾åˆ° {len(products)} ä¸ªäº§å“ï¼Œå¼€å§‹æå–å–å®¶ä¿¡æ¯...")
            
            # æå–å–å®¶ä¿¡æ¯
            def seller_progress(message):
                self.update_status(message)
            
            results = self.scraper.get_seller_info_batch(
                products,
                progress_callback=seller_progress,
                stop_flag=stop_flag
            )
            
            # æ›´æ–°UI
            successful_extractions = 0
            for result in results:
                if not self.is_scraping:
                    break
                self.root.after(0, self.add_result_to_tree, result)
                if result.get('seller_name', 'æœªçŸ¥') not in ['æœªçŸ¥', 'è·å–å¤±è´¥']:
                    successful_extractions += 1
            
            self.results = results
            self.root.after(0, self.scraping_finished, len(results), successful_extractions)
            
        except Exception as e:
            self.root.after(0, self.scraping_error, str(e))
    
    def add_result_to_tree(self, result):
        """æ·»åŠ ç»“æœåˆ°è¡¨æ ¼"""
        self.tree.insert('', tk.END, values=(
            result['product_title'][:50] + '...' if len(result['product_title']) > 50 else result['product_title'],
            result['price'],
            result['seller_name'],
            result['business_name'],
            result['store_name'],
            result['phone'],
            result['address'][:30] + '...' if len(result['address']) > 30 else result['address'],
            result['representative']
        ))
    
    def scraping_finished(self, total_results, successful_extractions):
        """çˆ¬å–å®Œæˆ"""
        self.is_scraping = False
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.export_button.config(state=tk.NORMAL if total_results > 0 else tk.DISABLED)
        self.progress_bar.stop()
        
        self.update_status(f"å®Œæˆï¼å…±æå– {total_results} ä¸ªäº§å“ï¼ŒæˆåŠŸè·å– {successful_extractions} ä¸ªå–å®¶ä¿¡æ¯")
    
    def scraping_error(self, error_message):
        """çˆ¬å–å‡ºé”™"""
        self.is_scraping = False
        self.start_button.config(state=tk.NORMAL)
        self.stop_button.config(state=tk.DISABLED)
        self.progress_bar.stop()
        
        self.update_status(f"å‡ºé”™: {error_message}")
        messagebox.showerror("é”™è¯¯", f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:\n{error_message}")
    
    def update_status(self, message):
        """æ›´æ–°çŠ¶æ€"""
        self.status_var.set(message)
        self.root.update_idletasks()
    
    def clear_results(self):
        """æ¸…ç©ºç»“æœ"""
        for item in self.tree.get_children():
            self.tree.delete(item)
        self.results = []
        self.update_status("å°±ç»ª")
    
    def export_data(self):
        """å¯¼å‡ºæ•°æ®"""
        if not self.results:
            messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        file_path = filedialog.asksaveasfilename(
            defaultextension=".xlsx",
            filetypes=[("Excelæ–‡ä»¶", "*.xlsx"), ("CSVæ–‡ä»¶", "*.csv")],
            title="ä¿å­˜æ•°æ®"
        )
        
        if file_path:
            try:
                df = pd.DataFrame(self.results)
                
                if file_path.endswith('.xlsx'):
                    df.to_excel(file_path, index=False)
                else:
                    df.to_csv(file_path, index=False, encoding='utf-8-sig')
                
                messagebox.showinfo("æˆåŠŸ", f"æ•°æ®å·²å¯¼å‡ºåˆ°: {file_path}")
                
            except Exception as e:
                messagebox.showerror("é”™è¯¯", f"å¯¼å‡ºå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    root = tk.Tk()
    app = SimplifiedScraperGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()
