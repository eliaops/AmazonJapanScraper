#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon Japan å–å®¶ä¿¡æ¯æå–å·¥å…· - ç»ˆæç‰ˆ v4.0
Ultimate Amazon Japan Seller Information Extractor v4.0

æ–°åŠŸèƒ½ï¼š
- æ‰©å¤§å…³é”®è¯æœç´¢èŒƒå›´ï¼Œæ”¯æŒæ›´å¤šå°å•†å“
- æ— é™åˆ¶è¿ç»­æœç´¢ï¼Œæƒ³æœå¤šä¹…æœå¤šä¹…
- å®æ—¶ä¿å­˜åŠŸèƒ½ï¼Œä¸€è¾¹æœç´¢ä¸€è¾¹ä¿å­˜
- è¿›ä¸€æ­¥ä¼˜åŒ–çš„å–å®¶ä¿¡æ¯æå–ç®—æ³•
- æ”¯æŒåå°è¿è¡Œï¼Œå¯ä»¥ç¦»å¼€æ¡Œé¢
"""

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import re
import threading
import os
from datetime import datetime
from urllib.parse import urljoin, quote
import json
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class UltimateAmazonScraper:
    """ç»ˆæç‰ˆAmazonæ—¥æœ¬ç«™çˆ¬è™«"""
    
    def __init__(self):
        self.base_url = "https://www.amazon.co.jp"
        self.session = requests.Session()
        
        # æ™ºèƒ½User-Agentæ±  - æ¨¡æ‹Ÿä¸åŒæµè§ˆå™¨å’Œè®¾å¤‡
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        
        # æ™ºèƒ½è¯·æ±‚å¤´é…ç½®
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
        }
        
        # ä¼šè¯çŠ¶æ€ç®¡ç†
        self.session_initialized = False
        self.last_request_time = 0
        self.request_count = 0
        self.current_user_agent_index = 0
        
        # é…ç½®è¿æ¥æ± å’Œé‡è¯•ç­–ç•¥ - ä¼˜åŒ–503å¤„ç†
        retry_strategy = Retry(
            total=2,  # å‡å°‘é‡è¯•æ¬¡æ•°
            backoff_factor=2,  # å¢åŠ é€€é¿æ—¶é—´
            status_forcelist=[429, 500, 502, 504],  # ç§»é™¤503ï¼Œç›´æ¥å¤±è´¥è€Œä¸é‡è¯•
        )
        adapter = HTTPAdapter(
            pool_connections=20,
            pool_maxsize=50,
            max_retries=retry_strategy
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # æ€§èƒ½é…ç½® - æ™ºèƒ½åŒ–è®¾ç½®
        self.max_concurrent_requests = 1  # å•çº¿ç¨‹é¿å…æ£€æµ‹
        self.request_delay_range = (3.0, 8.0)  # æ›´äººæ€§åŒ–çš„å»¶è¿Ÿ
        self.batch_size = 3  # æ›´å°çš„æ‰¹å¤„ç†å¤§å°
        
        # æ™ºèƒ½åæ£€æµ‹é…ç½®
        self.max_requests_per_session = 20  # æ¯ä¸ªä¼šè¯æœ€å¤§è¯·æ±‚æ•°
        self.session_cooldown_time = 30  # ä¼šè¯å†·å´æ—¶é—´(ç§’)
        self.user_agent_rotation_interval = 5  # User-Agentè½®æ¢é—´éš”
        
        # æœç´¢ä¼˜åŒ–é…ç½®
        self.search_strategies = [
            'default',      # é»˜è®¤æœç´¢
            'category',     # åˆ†ç±»æœç´¢
            'brand',        # å“ç‰Œæœç´¢
            'price_range',  # ä»·æ ¼åŒºé—´æœç´¢
        ]
        
        # æ‰©å±•çš„æœç´¢å‚æ•°
        self.search_params_variants = [
            {},  # é»˜è®¤
            {'sort': 'price-asc-rank'},  # ä»·æ ¼ä½åˆ°é«˜
            {'sort': 'price-desc-rank'}, # ä»·æ ¼é«˜åˆ°ä½
            {'sort': 'review-rank'},     # è¯„ä»·æ’åº
            {'sort': 'date-desc-rank'},  # æœ€æ–°å•†å“
        ]
        
        # å®æ—¶ä¿å­˜é…ç½®
        self.auto_save_interval = 50  # æ¯50ä¸ªäº§å“è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡
        self.save_directory = "amazon_data"
        self.ensure_save_directory()
        
        # æœç´¢çŠ¶æ€
        self.is_searching = False
        self.total_products_found = 0
        self.total_sellers_extracted = 0
        self.current_save_file = None
        
    def ensure_save_directory(self):
        """ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.save_directory):
            os.makedirs(self.save_directory)
    
    def _initialize_session(self):
        """æ™ºèƒ½ä¼šè¯åˆå§‹åŒ– - æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º"""
        if self.session_initialized:
            return True
            
        try:
            print("ğŸ”„ åˆå§‹åŒ–æ™ºèƒ½ä¼šè¯...")
            
            # 1. è®¾ç½®å½“å‰User-Agent
            current_ua = self.user_agents[self.current_user_agent_index]
            headers = self.base_headers.copy()
            headers['User-Agent'] = current_ua
            self.session.headers.update(headers)
            
            # 2. å…ˆè®¿é—®Amazoné¦–é¡µå»ºç«‹ä¼šè¯
            print("   ğŸ“± è®¿é—®Amazoné¦–é¡µ...")
            response = self.session.get(self.base_url, timeout=15)
            
            if response.status_code != 200:
                print(f"   âŒ é¦–é¡µè®¿é—®å¤±è´¥: {response.status_code}")
                return False
                
            # 3. æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆè¡Œä¸º - è®¿é—®å‡ ä¸ªå¸¸è§é¡µé¢
            common_pages = [
                '/gp/bestsellers',  # ç•…é”€å•†å“
                '/gp/new-releases', # æ–°å“å‘å¸ƒ
            ]
            
            for page in common_pages:
                time.sleep(random.uniform(2, 4))  # äººæ€§åŒ–å»¶è¿Ÿ
                try:
                    self.session.get(f"{self.base_url}{page}", timeout=10)
                    print(f"   âœ… è®¿é—®é¡µé¢: {page}")
                except:
                    pass  # å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­
            
            # 4. ç­‰å¾…ä¸€æ®µæ—¶é—´æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
            wait_time = random.uniform(3, 6)
            print(f"   â±ï¸ ç­‰å¾… {wait_time:.1f}ç§’...")
            time.sleep(wait_time)
            
            self.session_initialized = True
            self.request_count = 0
            print("   âœ… ä¼šè¯åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"   âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _smart_request(self, url, params=None, **kwargs):
        """æ™ºèƒ½è¯·æ±‚æ–¹æ³• - åŒ…å«åæ£€æµ‹æœºåˆ¶"""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–ä¼šè¯
        if not self.session_initialized:
            if not self._initialize_session():
                raise Exception("ä¼šè¯åˆå§‹åŒ–å¤±è´¥")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢User-Agent
        if self.request_count > 0 and self.request_count % self.user_agent_rotation_interval == 0:
            self.current_user_agent_index = (self.current_user_agent_index + 1) % len(self.user_agents)
            new_ua = self.user_agents[self.current_user_agent_index]
            self.session.headers.update({'User-Agent': new_ua})
            print(f"ğŸ”„ è½®æ¢User-Agent: {new_ua[:50]}...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®ä¼šè¯
        if self.request_count >= self.max_requests_per_session:
            print(f"ğŸ”„ è¾¾åˆ°æœ€å¤§è¯·æ±‚æ•°({self.max_requests_per_session})ï¼Œé‡ç½®ä¼šè¯...")
            self._reset_session()
            if not self._initialize_session():
                raise Exception("ä¼šè¯é‡ç½®å¤±è´¥")
        
        # æ™ºèƒ½å»¶è¿Ÿ - åŸºäºä¸Šæ¬¡è¯·æ±‚æ—¶é—´
        current_time = time.time()
        if self.last_request_time > 0:
            elapsed = current_time - self.last_request_time
            min_delay = self.request_delay_range[0]
            if elapsed < min_delay:
                additional_delay = min_delay - elapsed + random.uniform(0, 2)
                print(f"â±ï¸ æ™ºèƒ½å»¶è¿Ÿ: {additional_delay:.1f}ç§’")
                time.sleep(additional_delay)
        
        # æ‰§è¡Œè¯·æ±‚
        try:
            # æ·»åŠ éšæœºåŒ–çš„è¯·æ±‚å¤´
            headers = kwargs.get('headers', {})
            if 'Referer' not in headers and self.request_count > 0:
                headers['Referer'] = self.base_url
            kwargs['headers'] = headers
            
            response = self.session.get(url, params=params, **kwargs)
            
            # ç‰¹æ®Šå¤„ç†503é”™è¯¯
            if response.status_code == 503:
                print("âš ï¸ é‡åˆ°503é”™è¯¯ï¼Œå¯åŠ¨æ™ºèƒ½æ¢å¤...")
                self._handle_503_error()
                # é‡è¯•ä¸€æ¬¡
                time.sleep(random.uniform(10, 20))
                response = self.session.get(url, params=params, **kwargs)
            
            self.last_request_time = time.time()
            self.request_count += 1
            
            return response
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½è¯·æ±‚å¤±è´¥: {e}")
            raise
    
    def _reset_session(self):
        """é‡ç½®ä¼šè¯"""
        self.session.close()
        self.session = requests.Session()
        
        # é‡æ–°é…ç½®é€‚é…å™¨
        retry_strategy = Retry(
            total=2,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 504],
        )
        adapter = HTTPAdapter(
            pool_connections=20,
            pool_maxsize=50,
            max_retries=retry_strategy
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        self.session_initialized = False
        self.request_count = 0
        print("ğŸ”„ ä¼šè¯å·²é‡ç½®")
    
    def _handle_503_error(self):
        """å¤„ç†503é”™è¯¯çš„æ™ºèƒ½ç­–ç•¥"""
        print("ğŸ›¡ï¸ å¯åŠ¨503é”™è¯¯å¤„ç†ç­–ç•¥...")
        
        # 1. è½®æ¢User-Agent
        self.current_user_agent_index = (self.current_user_agent_index + 1) % len(self.user_agents)
        new_ua = self.user_agents[self.current_user_agent_index]
        self.session.headers.update({'User-Agent': new_ua})
        print(f"   ğŸ”„ è½®æ¢User-Agent")
        
        # 2. æ¸…é™¤å¯èƒ½çš„è¿½è¸ªcookie
        self.session.cookies.clear()
        print("   ğŸª æ¸…é™¤cookies")
        
        # 3. ç­‰å¾…å†·å´æ—¶é—´
        cooldown = random.uniform(self.session_cooldown_time, self.session_cooldown_time * 1.5)
        print(f"   â„ï¸ å†·å´ç­‰å¾…: {cooldown:.1f}ç§’")
        time.sleep(cooldown)
        
        # 4. é‡ç½®ä¼šè¯çŠ¶æ€
        self.session_initialized = False
        print("   âœ… 503é”™è¯¯å¤„ç†å®Œæˆ")
    
    def unlimited_search(self, keyword, progress_callback=None, stop_flag=None, save_callback=None):
        """
        æ— é™åˆ¶æœç´¢åŠŸèƒ½ - æ ¸å¿ƒæ”¹è¿›
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            progress_callback: è¿›åº¦å›è°ƒ
            stop_flag: åœæ­¢æ ‡å¿—
            save_callback: ä¿å­˜å›è°ƒ
        """
        self.is_searching = True
        self.total_products_found = 0
        self.total_sellers_extracted = 0
        
        # åˆ›å»ºä¿å­˜æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_save_file = os.path.join(
            self.save_directory, 
            f"amazon_search_{keyword}_{timestamp}.xlsx"
        )
        
        all_products = []
        all_sellers = []
        page = 1
        consecutive_empty_pages = 0
        max_consecutive_empty = 5  # è¿ç»­5é¡µæ— ç»“æœåˆ™åœæ­¢
        
        try:
            while self.is_searching:
                # æ£€æŸ¥åœæ­¢æ¡ä»¶ï¼šstop_flag()è¿”å›Trueè¡¨ç¤ºç»§ç»­æœç´¢ï¼ŒFalseè¡¨ç¤ºåœæ­¢
                if stop_flag and not stop_flag():
                    self.is_searching = False  # é‡è¦ï¼šè®¾ç½®çŠ¶æ€ä¸ºFalseä»¥é€€å‡ºwhileå¾ªç¯
                    break
                
                if progress_callback:
                    progress_callback(f"ğŸ” æœç´¢ç¬¬ {page} é¡µ | å·²æ‰¾åˆ° {self.total_products_found} ä¸ªäº§å“ | å·²æå– {self.total_sellers_extracted} ä¸ªå–å®¶")
                
                # ä½¿ç”¨å¤šç§æœç´¢ç­–ç•¥
                page_products = []
                for strategy_idx, search_params in enumerate(self.search_params_variants):
                    if stop_flag and not stop_flag():
                        break
                    
                    products = self._search_page_with_strategy(
                        keyword, page, search_params, strategy_idx
                    )
                    
                    if products:
                        page_products.extend(products)
                        consecutive_empty_pages = 0
                    
                    # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    time.sleep(random.uniform(0.5, 1.0))
                
                # å»é‡å¤„ç†
                unique_products = self._deduplicate_products(page_products, all_products)
                
                if not unique_products:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= max_consecutive_empty:
                        if progress_callback:
                            progress_callback(f"âš ï¸ è¿ç»­ {max_consecutive_empty} é¡µæ— æ–°äº§å“ï¼Œæœç´¢å¯èƒ½å·²å®Œæˆ")
                        self.is_searching = False  # è®¾ç½®çŠ¶æ€ä¸ºFalseä»¥é€€å‡ºwhileå¾ªç¯
                        break
                else:
                    all_products.extend(unique_products)
                    self.total_products_found = len(all_products)
                    
                    # æ‰¹é‡æå–å–å®¶ä¿¡æ¯
                    if unique_products:
                        sellers = self._extract_sellers_batch(
                            unique_products, progress_callback, stop_flag
                        )
                        all_sellers.extend(sellers)
                        self.total_sellers_extracted = len(all_sellers)
                
                # å®æ—¶ä¿å­˜
                if len(all_products) % self.auto_save_interval == 0 and all_products:
                    self._save_data_realtime(all_products, all_sellers, save_callback)
                
                page += 1
                
                # é¡µé¢é—´å»¶è¿Ÿ
                delay = random.uniform(*self.request_delay_range)
                time.sleep(delay)
                
                # å†…å­˜ç®¡ç†
                if page % 20 == 0:
                    gc.collect()
        
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
        finally:
            # æœ€ç»ˆä¿å­˜
            if all_products:
                self._save_data_final(all_products, all_sellers, save_callback)
            
            self.is_searching = False
            
            if progress_callback:
                progress_callback(f"âœ… æœç´¢å®Œæˆï¼æ€»è®¡æ‰¾åˆ° {self.total_products_found} ä¸ªäº§å“ï¼Œæå– {self.total_sellers_extracted} ä¸ªå–å®¶ä¿¡æ¯")
        
        return all_products, all_sellers
    
    def _search_page_with_strategy(self, keyword, page, search_params, strategy_idx):
        """ä½¿ç”¨ç‰¹å®šç­–ç•¥æœç´¢é¡µé¢"""
        try:
            # æ„å»ºæœç´¢URLå’Œå‚æ•°
            base_params = {
                'k': keyword,
                'page': page,
                'ref': f'sr_pg_{page}'
            }
            base_params.update(search_params)
            
            # æ·»åŠ éšæœºåŒ–å‚æ•°ä»¥è·å¾—æ›´å¤šç»“æœ
            if strategy_idx > 0:
                base_params['qid'] = str(int(time.time()))
            
            search_url = f"{self.base_url}/s"
            
            # ä½¿ç”¨æ™ºèƒ½è¯·æ±‚æ–¹æ³•
            response = self._smart_request(search_url, params=base_params, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ‰©å±•çš„äº§å“é€‰æ‹©å™¨ - æ”¯æŒæ›´å¤šå•†å“ç±»å‹
            product_selectors = [
                'div[data-component-type="s-search-result"]',
                '.s-result-item[data-component-type="s-search-result"]',
                '.s-search-result',
                '.sg-col-inner .s-widget-container',
                '[data-asin]:not([data-asin=""])',
                '.s-card-container',
                '.AdHolder',  # å¹¿å‘Šå•†å“
                '.s-sponsored-list-item',  # èµåŠ©å•†å“
            ]
            
            products = []
            for selector in product_selectors:
                items = soup.select(selector)
                for item in items:
                    product_info = self._extract_product_info_enhanced(item)
                    if product_info:
                        products.append(product_info)
            
            return products
            
        except Exception as e:
            print(f"æœç´¢ç­–ç•¥ {strategy_idx} ç¬¬ {page} é¡µå¤±è´¥: {e}")
            return []
    
    def _extract_product_info_enhanced(self, product_element):
        """å¢å¼ºçš„äº§å“ä¿¡æ¯æå–"""
        try:
            if not product_element:
                return None
            
            # è·å–ASINï¼ˆAmazonæ ‡å‡†è¯†åˆ«å·ï¼‰
            asin = product_element.get('data-asin', '')
            
            # å¤šç§é“¾æ¥æå–ç­–ç•¥
            link_selectors = [
                'h2 a',
                '.a-link-normal',
                'a[href*="/dp/"]',
                'a[href*="/gp/product/"]',
                '.s-link-style a',
                '.a-size-mini a',
                '.a-size-base-plus a'
            ]
            
            link_element = None
            for selector in link_selectors:
                link_element = product_element.select_one(selector)
                if link_element:
                    break
            
            if not link_element:
                return None
            
            product_url = urljoin(self.base_url, link_element.get('href', ''))
            # ä¿®å¤ï¼šæ¥å—æ›´å¤šç±»å‹çš„äº§å“é“¾æ¥ï¼ŒåŒ…æ‹¬èµåŠ©å•†å“é“¾æ¥
            if not product_url or not any(pattern in product_url for pattern in ['/dp/', '/gp/product/', '/sspa/click', '/gp/slredirect/']):
                return None
            
            # å¤šç§æ ‡é¢˜æå–ç­–ç•¥
            title_selectors = [
                'h2 a span',
                '.a-size-mini span',
                '.a-size-base-plus',
                '.s-size-mini',
                'h2 span',
                '.a-link-normal span'
            ]
            
            title = "æœªçŸ¥äº§å“"
            for selector in title_selectors:
                title_element = product_element.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    if title and len(title) > 5:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                        break
            
            # å¤šç§ä»·æ ¼æå–ç­–ç•¥
            price_selectors = [
                '.a-price .a-offscreen',
                '.a-price-whole',
                '.a-price-range .a-offscreen',
                '.a-price-symbol + .a-price-whole',
                '.s-price-instructions-style .a-price .a-offscreen',
                '.a-price-range',
                'span[data-a-color="price"]'
            ]
            
            price = "ä»·æ ¼æœªçŸ¥"
            for selector in price_selectors:
                price_element = product_element.select_one(selector)
                if price_element:
                    price_text = price_element.get_text(strip=True)
                    if price_text and any(char.isdigit() for char in price_text):
                        price = price_text
                        break
            
            # æå–è¯„åˆ†å’Œè¯„ä»·æ•°
            rating_element = product_element.select_one('.a-icon-alt')
            rating = rating_element.get_text(strip=True) if rating_element else ""
            
            review_count_element = product_element.select_one('.a-size-base')
            review_count = review_count_element.get_text(strip=True) if review_count_element else ""
            
            # æå–å›¾ç‰‡URL
            img_element = product_element.select_one('img')
            image_url = img_element.get('src', '') if img_element else ""
            
            return {
                'asin': asin,
                'title': title[:150],  # å¢åŠ æ ‡é¢˜é•¿åº¦é™åˆ¶
                'price': price,
                'rating': rating,
                'review_count': review_count,
                'image_url': image_url,
                'url': product_url,
                'extracted_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"æå–äº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _deduplicate_products(self, new_products, existing_products):
        """å»é‡å¤„ç†"""
        existing_urls = {p['url'] for p in existing_products}
        existing_asins = {p.get('asin', '') for p in existing_products if p.get('asin')}
        
        unique_products = []
        for product in new_products:
            if (product['url'] not in existing_urls and 
                product.get('asin', '') not in existing_asins):
                unique_products.append(product)
        
        return unique_products
    
    def _extract_sellers_batch(self, products, progress_callback=None, stop_flag=None):
        """æ‰¹é‡æå–å–å®¶ä¿¡æ¯"""
        sellers = []
        
        with ThreadPoolExecutor(max_workers=self.max_concurrent_requests) as executor:
            # æäº¤ä»»åŠ¡
            future_to_product = {
                executor.submit(self._get_seller_info_ultimate, product['url']): product 
                for product in products
            }
            
            # å¤„ç†ç»“æœ
            for future in as_completed(future_to_product):
                if stop_flag and not stop_flag():
                    break
                
                product = future_to_product[future]
                try:
                    seller_info = future.result(timeout=30)
                    if seller_info:
                        seller_info.update({
                            'product_title': product['title'],
                            'product_url': product['url'],
                            'product_asin': product.get('asin', ''),
                            'extracted_at': datetime.now().isoformat()
                        })
                        sellers.append(seller_info)
                        
                        if progress_callback:
                            progress_callback(f"âœ… å·²æå–å–å®¶: {seller_info.get('seller_name', 'æœªçŸ¥')} | æ€»è®¡: {len(sellers)}")
                
                except Exception as e:
                    if progress_callback:
                        progress_callback(f"âš ï¸ æå–å–å®¶ä¿¡æ¯å¤±è´¥: {product['title'][:30]}... - {e}")
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                time.sleep(random.uniform(0.3, 0.8))
        
        return sellers
    
    def _get_seller_info_ultimate(self, product_url):
        """ç»ˆæç‰ˆå–å®¶ä¿¡æ¯æå–"""
        try:
            # è·³è¿‡èµåŠ©å•†å“é“¾æ¥ï¼Œé¿å…é‡å®šå‘å¯¼è‡´çš„æŒ‚èµ·é—®é¢˜
            if '/sspa/click' in product_url or '/gp/slredirect/' in product_url:
                return {
                    'seller_name': 'èµåŠ©å•†å“',
                    'seller_url': '',
                    'business_name': '',
                    'phone': '',
                    'address': '',
                    'representative_name': '',
                    'store_name': '',
                    'email': '',
                    'fax': ''
                }
            
            # ç¬¬ä¸€æ­¥ï¼šè·å–äº§å“é¡µé¢ - ä½¿ç”¨æ™ºèƒ½è¯·æ±‚
            response = self._smart_request(product_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å–å®¶ä¿¡æ¯çš„å¤šç§ç­–ç•¥
            seller_info = {}
            
            # ç­–ç•¥1ï¼šæŸ¥æ‰¾"å‡ºå”®æ–¹"ä¿¡æ¯ - å¢å¼ºç‰ˆ
            seller_selectors = [
                '#merchant-info',
                '#merchantInfoFeature_feature_div',
                '#tabular-buybox',
                '#buybox',
                '.a-section.a-spacing-small:contains("å‡ºå”®æ–¹")',
                '.a-section:contains("è²©å£²")',
                '.a-section:contains("Sold by")',
                '.a-section:contains("é”€å”®")',
                '#tabular-buybox .a-section',
                '#buybox-see-all-buying-choices',
                '.a-box-group .a-box',
                'span:contains("å‡ºå”®æ–¹")',
                'span:contains("è²©å£²")',
                'span:contains("Sold by")'
            ]
            
            seller_name = None
            seller_url = None
            
            for selector in seller_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text()
                    if any(keyword in text for keyword in ['å‡ºå”®æ–¹', 'è²©å£²', 'Sold by', 'é”€å”®']):
                        # æŸ¥æ‰¾å–å®¶é“¾æ¥
                        seller_link = element.select_one('a[href*="/sp?"]')
                        if seller_link:
                            seller_name = seller_link.get_text(strip=True)
                            seller_url = urljoin(self.base_url, seller_link.get('href'))
                            break
                
                if seller_name and seller_url:
                    break
            
            # ç­–ç•¥2ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not seller_name:
                # æŸ¥æ‰¾buyboxä¸­çš„å–å®¶ä¿¡æ¯
                buybox_selectors = [
                    '#buybox .a-section a[href*="/sp?"]',
                    '.a-box-group a[href*="/sp?"]',
                    '#merchant-info a'
                ]
                
                for selector in buybox_selectors:
                    seller_link = soup.select_one(selector)
                    if seller_link:
                        seller_name = seller_link.get_text(strip=True)
                        seller_url = urljoin(self.base_url, seller_link.get('href'))
                        break
            
            # å¦‚æœæ‰¾åˆ°å–å®¶é“¾æ¥ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
            if seller_url:
                detailed_info = self._get_detailed_seller_info_ultimate(seller_url)
                seller_info.update(detailed_info)
            
            # åŸºæœ¬ä¿¡æ¯
            seller_info.update({
                'seller_name': seller_name or 'æœªçŸ¥å–å®¶',
                'seller_url': seller_url or '',
            })
            
            return seller_info
            
        except Exception as e:
            print(f"è·å–å–å®¶ä¿¡æ¯å¤±è´¥ {product_url}: {e}")
            return None
    
    def _get_detailed_seller_info_ultimate(self, seller_url):
        """ç»ˆæç‰ˆè¯¦ç»†å–å®¶ä¿¡æ¯æå–"""
        try:
            response = self._smart_request(seller_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å››å±‚æå–ç­–ç•¥
            info = {}
            
            # ç¬¬ä¸€å±‚ï¼šæ™ºèƒ½å…³é”®è¯æå–
            smart_info = self._smart_extract_seller_info_ultimate(soup)
            
            # ç¬¬äºŒå±‚ï¼šHTMLç»“æ„æå–
            structured_info = self._extract_from_html_structure_ultimate(soup)
            
            # ç¬¬ä¸‰å±‚ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–
            regex_info = self._extract_with_regex_ultimate(soup.get_text())
            
            # ç¬¬å››å±‚ï¼šæ·±åº¦æ–‡æœ¬åˆ†æ
            deep_info = self._deep_text_analysis(soup.get_text())
            
            # åˆå¹¶ç»“æœï¼Œä¼˜å…ˆçº§ï¼šæ™ºèƒ½ > ç»“æ„ > æ­£åˆ™ > æ·±åº¦åˆ†æ
            fields = ['business_name', 'phone', 'address', 'representative', 'store_name', 'email', 'fax']
            
            for field in fields:
                info[field] = (
                    smart_info.get(field) or 
                    structured_info.get(field) or 
                    regex_info.get(field) or 
                    deep_info.get(field) or 
                    ''
                )
            
            # æ¸…ç†å’ŒéªŒè¯
            info = self._clean_seller_info_ultimate(info)
            
            return info
            
        except Exception as e:
            print(f"è·å–è¯¦ç»†å–å®¶ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _smart_extract_seller_info_ultimate(self, soup):
        """ç»ˆæç‰ˆæ™ºèƒ½å…³é”®è¯æå–"""
        info = {}
        
        # æ‰©å±•çš„å…³é”®è¯æ˜ å°„
        field_keywords = {
            'business_name': [
                'Business Name', 'business name', 'BUSINESS NAME',
                'ä¼šç¤¾å', 'å•†å·', 'ä¼ä¸šåç§°', 'å…¬å¸åç§°', 'æ³•äººåç§°',
                'Company Name', 'company name', 'COMPANY NAME',
                'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'Corporation', 'Corp', 'Ltd', 'Inc',
                'äº‹æ¥­è€…å', 'è²©å£²æ¥­è€…', 'è²©å£²äº‹æ¥­è€…å'
            ],
            'phone': [
                'å’¨è¯¢ç”¨ç”µè¯å·ç ', 'ç”µè¯å·ç ', 'é›»è©±ç•ªå·', 'TEL', 'Tel', 'tel',
                'Phone', 'phone', 'PHONE', 'Telephone', 'telephone',
                'è”ç³»ç”µè¯', 'å’¨è¯¢ç”µè¯', 'å®¢æœç”µè¯', 'æœåŠ¡ç”µè¯',
                'é›»è©±', 'ã§ã‚“ã‚', 'ï¼´ï¼¥ï¼¬', 'â˜', 'ğŸ“'
            ],
            'address': [
                'åœ°å€', 'ä½æ‰€', 'æ‰€åœ¨åœ°', 'Address', 'address', 'ADDRESS',
                'è”ç³»åœ°å€', 'å…¬å¸åœ°å€', 'è¥ä¸šåœ°å€', 'äº‹ä¸šæ‰€æ‰€åœ¨åœ°',
                'æœ¬ç¤¾æ‰€åœ¨åœ°', 'äº‹å‹™æ‰€', 'äº‹åŠ¡æ‰€', 'å–¶æ¥­æ‰€'
            ],
            'representative': [
                'è´­ç‰©ä»£è¡¨çš„å§“å', 'ä»£è¡¨è€…', 'ä»£è¡¨å–ç· å½¹', 'è²¬ä»»è€…', 'è´Ÿè´£äºº',
                'Representative', 'representative', 'REPRESENTATIVE',
                'è”ç³»äºº', 'ä»£è¡¨äºº', 'æ‹…å½“è€…', 'è²¬ä»»è€…æ°å',
                'ä»£è¡¨è€…æ°å', 'ä»£è¡¨å–ç· å½¹æ°å', 'CEO', 'President'
            ],
            'store_name': [
                'å•†åº—å', 'åº—èˆ—å', 'ã‚·ãƒ§ãƒƒãƒ—å', 'Store Name', 'store name',
                'Shop Name', 'shop name', 'åº—é“ºåç§°', 'åº—å',
                'ã‚¹ãƒˆã‚¢å', 'è²©å£²åº—å', 'Seller Name'
            ],
            'email': [
                'Email', 'email', 'E-mail', 'e-mail', 'EMAIL',
                'ãƒ¡ãƒ¼ãƒ«', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', 'ç”µå­é‚®ä»¶', 'é‚®ç®±',
                'è”ç³»é‚®ç®±', 'Contact Email', '@'
            ],
            'fax': [
                'Fax', 'fax', 'FAX', 'ãƒ•ã‚¡ãƒƒã‚¯ã‚¹', 'ãƒ•ã‚¡ã‚¯ã‚¹',
                'ä¼ çœŸ', 'ä¼ çœŸå·ç ', 'Fax Number'
            ]
        }
        
        # æŸ¥æ‰¾åŒ…å«å–å®¶ä¿¡æ¯çš„åŒºåŸŸ
        info_sections = soup.find_all(['div', 'section', 'table', 'dl', 'ul'], 
                                     string=re.compile(r'è¯¦å°½çš„å–å®¶ä¿¡æ¯|è¯¦æ°‘çš„å–å®¶ä¿¡æ¯|seller.*info|äº‹æ¥­è€…æƒ…å ±|è²©å£²æ¥­è€…æƒ…å ±', re.I))
        
        if not info_sections:
            # æ‰©å¤§æœç´¢èŒƒå›´
            info_sections = soup.find_all(['div', 'section', 'table', 'dl', 'ul'])
        
        for section in info_sections[:10]:  # é™åˆ¶æœç´¢èŒƒå›´
            section_text = section.get_text() if section else ""
            
            for field, keywords in field_keywords.items():
                if info.get(field):  # å·²ç»æ‰¾åˆ°è¯¥å­—æ®µ
                    continue
                
                for keyword in keywords:
                    if keyword.lower() in section_text.lower():
                        value = self._extract_value_near_keyword_ultimate(section_text, keyword, field)
                        if value and self._validate_extracted_value_ultimate(field, value):
                            info[field] = value
                            break
                
                if info.get(field):
                    break
        
        return info
    
    def _extract_from_html_structure_ultimate(self, soup):
        """ç»ˆæç‰ˆHTMLç»“æ„æå–"""
        info = {}
        
        # æŸ¥æ‰¾è¡¨æ ¼ç»“æ„
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key_text = cells[0].get_text().strip().lower()
                    value_text = cells[1].get_text().strip()
                    
                    # åŒ¹é…å­—æ®µ
                    if not info.get('business_name') and any(k in key_text for k in ['business', 'ä¼šç¤¾', 'å•†å·', 'æ³•äºº', 'äº‹æ¥­è€…']):
                        info['business_name'] = value_text
                    elif not info.get('phone') and any(k in key_text for k in ['ç”µè¯', 'tel', 'phone', 'é›»è©±']):
                        info['phone'] = value_text
                    elif not info.get('address') and any(k in key_text for k in ['åœ°å€', 'ä½æ‰€', 'address', 'æ‰€åœ¨åœ°']):
                        info['address'] = value_text
                    elif not info.get('representative') and any(k in key_text for k in ['ä»£è¡¨', 'representative', 'è²¬ä»»è€…', 'æ‹…å½“']):
                        info['representative'] = value_text
                    elif not info.get('store_name') and any(k in key_text for k in ['åº—', 'store', 'shop', 'ã‚·ãƒ§ãƒƒãƒ—']):
                        info['store_name'] = value_text
                    elif not info.get('email') and any(k in key_text for k in ['email', 'mail', 'ãƒ¡ãƒ¼ãƒ«', '@']):
                        info['email'] = value_text
                    elif not info.get('fax') and any(k in key_text for k in ['fax', 'ãƒ•ã‚¡ãƒƒã‚¯ã‚¹', 'ä¼ çœŸ']):
                        info['fax'] = value_text
        
        # æŸ¥æ‰¾å®šä¹‰åˆ—è¡¨
        dls = soup.find_all('dl')
        for dl in dls:
            dts = dl.find_all('dt')
            dds = dl.find_all('dd')
            
            for dt, dd in zip(dts, dds):
                key_text = dt.get_text().strip().lower()
                value_text = dd.get_text().strip()
                
                if not info.get('business_name') and any(k in key_text for k in ['business', 'ä¼šç¤¾', 'å•†å·']):
                    info['business_name'] = value_text
                elif not info.get('phone') and any(k in key_text for k in ['ç”µè¯', 'tel', 'phone']):
                    info['phone'] = value_text
                # ... å…¶ä»–å­—æ®µç±»ä¼¼å¤„ç†
        
        return info
    
    def _extract_with_regex_ultimate(self, text):
        """ç»ˆæç‰ˆæ­£åˆ™è¡¨è¾¾å¼æå–"""
        info = {}
        
        # å¢å¼ºçš„æ­£åˆ™æ¨¡å¼
        patterns = {
            'business_name': [
                r'Business\s*Name[ï¼š:\s]*([^\n\r]{3,80})',
                r'ä¼šç¤¾å[ï¼š:\s]*([^\n\r]{3,80})',
                r'å•†å·[ï¼š:\s]*([^\n\r]{3,80})',
                r'äº‹æ¥­è€…å[ï¼š:\s]*([^\n\r]{3,80})',
                r'æ³•äººåç§°[ï¼š:\s]*([^\n\r]{3,80})',
                r'([A-Za-z\s]{3,50}(?:æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|Company|Ltd|Corporation|Inc|Corp))',
            ],
            'phone': [
                r'å’¨è¯¢ç”¨ç”µè¯å·ç [ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'é›»è©±ç•ªå·[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'TEL[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'Phone[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'(\+?\d{1,4}[-\s]?\d{10,12})',
                r'(\d{2,4}[-\s]\d{4}[-\s]\d{4})',
                r'(\(\d{2,4}\)\s?\d{4}[-\s]?\d{4})',
            ],
            'address': [
                r'åœ°å€[ï¼š:\s]*([^\n\r]{10,150})',
                r'ä½æ‰€[ï¼š:\s]*([^\n\r]{10,150})',
                r'Address[ï¼š:\s]*([^\n\r]{10,150})',
                r'æ‰€åœ¨åœ°[ï¼š:\s]*([^\n\r]{10,150})',
                r'(ã€’\d{3}-\d{4}[^\n\r]{5,120})',
                r'(\d{6}[^\n\r]{8,120})',  # ä¸­å›½é‚®ç¼–æ ¼å¼
            ],
            'representative': [
                r'è´­ç‰©ä»£è¡¨çš„å§“å[ï¼š:\s]*([^\n\r]{2,40})',
                r'ä»£è¡¨è€…[ï¼š:\s]*([^\n\r]{2,40})',
                r'ä»£è¡¨å–ç· å½¹[ï¼š:\s]*([^\n\r]{2,40})',
                r'Representative[ï¼š:\s]*([^\n\r]{2,40})',
                r'è²¬ä»»è€…[ï¼š:\s]*([^\n\r]{2,40})',
                r'æ‹…å½“è€…[ï¼š:\s]*([^\n\r]{2,40})',
            ],
            'store_name': [
                r'å•†åº—å[ï¼š:\s]*([^\n\r]{2,50})',
                r'åº—èˆ—å[ï¼š:\s]*([^\n\r]{2,50})',
                r'Store\s*Name[ï¼š:\s]*([^\n\r]{2,50})',
                r'ã‚·ãƒ§ãƒƒãƒ—å[ï¼š:\s]*([^\n\r]{2,50})',
                r'è²©å£²åº—å[ï¼š:\s]*([^\n\r]{2,50})',
            ],
            'email': [
                r'Email[ï¼š:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'ãƒ¡ãƒ¼ãƒ«[ï¼š:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            ],
            'fax': [
                r'Fax[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'ãƒ•ã‚¡ãƒƒã‚¯ã‚¹[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
                r'ä¼ çœŸ[ï¼š:\s]*(\+?[\d\-\(\)\s]{8,25})',
            ]
        }
        
        for field, field_patterns in patterns.items():
            for pattern in field_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    value = match.group(1).strip()
                    if self._validate_extracted_value_ultimate(field, value):
                        info[field] = value
                        break
        
        return info
    
    def _deep_text_analysis(self, text):
        """æ·±åº¦æ–‡æœ¬åˆ†ææå–"""
        info = {}
        
        # ä½¿ç”¨æ›´å¤æ‚çš„æ–‡æœ¬åˆ†æ
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # åˆ†æä¸Šä¸‹æ–‡
            context = ' '.join(lines[max(0, i-2):i+3])  # å‰å2è¡Œçš„ä¸Šä¸‹æ–‡
            
            # ç”µè¯å·ç æ¨¡å¼åŒ¹é…
            if not info.get('phone'):
                phone_match = re.search(r'(\+?\d{1,4}[-\s]?\d{8,12})', line)
                if phone_match and any(keyword in context.lower() for keyword in ['ç”µè¯', 'tel', 'phone', 'é€£çµ¡']):
                    info['phone'] = phone_match.group(1)
            
            # é‚®ç®±æ¨¡å¼åŒ¹é…
            if not info.get('email'):
                email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', line)
                if email_match:
                    info['email'] = email_match.group(1)
            
            # åœ°å€æ¨¡å¼åŒ¹é…ï¼ˆåŒ…å«é‚®ç¼–ï¼‰
            if not info.get('address'):
                if re.search(r'ã€’\d{3}-\d{4}', line) or re.search(r'\d{6}', line):
                    # å¯èƒ½æ˜¯åœ°å€è¡Œï¼Œå–å½“å‰è¡Œå’Œä¸‹ä¸€è¡Œ
                    address_parts = [line]
                    if i + 1 < len(lines):
                        address_parts.append(lines[i + 1].strip())
                    potential_address = ' '.join(address_parts)
                    if len(potential_address) > 10:
                        info['address'] = potential_address
        
        return info
    
    def _extract_value_near_keyword_ultimate(self, text, keyword, field):
        """ç»ˆæç‰ˆå…³é”®è¯é™„è¿‘å€¼æå–"""
        try:
            # æ‰¾åˆ°å…³é”®è¯ä½ç½®
            keyword_pos = text.lower().find(keyword.lower())
            if keyword_pos == -1:
                return None
            
            # æå–å…³é”®è¯åçš„æ–‡æœ¬
            after_keyword = text[keyword_pos + len(keyword):].strip()
            
            # ç§»é™¤å¼€å¤´çš„åˆ†éš”ç¬¦
            after_keyword = re.sub(r'^[ï¼š:\s\-=]+', '', after_keyword)
            
            if field == 'phone':
                # ç”µè¯å·ç ç‰¹æ®Šå¤„ç†
                phone_patterns = [
                    r'\+?\d{1,4}[-\s]?\d{10,12}',
                    r'\+?\d{11,15}',
                    r'\d{2,4}[-\s]\d{4}[-\s]\d{4}',
                    r'\(\d{2,4}\)\s?\d{4}[-\s]?\d{4}',
                ]
                for pattern in phone_patterns:
                    match = re.search(pattern, after_keyword)
                    if match:
                        return match.group(0)
            elif field == 'email':
                # é‚®ç®±ç‰¹æ®Šå¤„ç†
                email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
                match = re.search(email_pattern, after_keyword)
                if match:
                    return match.group(1)
            elif field == 'address':
                # åœ°å€é€šå¸¸æ˜¯å¤šè¡Œçš„
                lines = after_keyword.split('\n')[:4]  # å–å‰4è¡Œ
                address_parts = []
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 2:
                        address_parts.append(line)
                        if len(' '.join(address_parts)) > 20:  # åœ°å€è¶³å¤Ÿé•¿
                            break
                return ' '.join(address_parts) if address_parts else None
            else:
                # å…¶ä»–å­—æ®µå–ç¬¬ä¸€è¡Œ
                first_line = after_keyword.split('\n')[0].strip()
                # ç§»é™¤å¯èƒ½çš„åç»­å­—æ®µæ ‡è¯†
                first_line = re.split(r'[ï¼š:]', first_line)[0].strip()
                return first_line if len(first_line) > 1 else None
            
        except Exception:
            return None
    
    def _validate_extracted_value_ultimate(self, field, value):
        """ç»ˆæç‰ˆå€¼éªŒè¯"""
        if not value or len(value.strip()) < 2:
            return False
        
        value = value.strip()
        
        if field == 'phone':
            # ç”µè¯å·ç å¿…é¡»åŒ…å«è¶³å¤Ÿçš„æ•°å­—
            digit_count = len(re.findall(r'\d', value))
            return 8 <= digit_count <= 20
        elif field == 'email':
            # é‚®ç®±æ ¼å¼éªŒè¯
            return bool(re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', value))
        elif field == 'address':
            # åœ°å€å¿…é¡»æœ‰ä¸€å®šé•¿åº¦ä¸”åŒ…å«æœ‰æ„ä¹‰å­—ç¬¦
            return len(value) >= 8 and bool(re.search(r'[\u4e00-\u9fff]|[a-zA-Z]', value))
        elif field == 'business_name':
            # å…¬å¸åä¸èƒ½å¤ªçŸ­æˆ–å¤ªé•¿
            return 3 <= len(value) <= 100
        elif field == 'representative':
            # ä»£è¡¨äººå§“åé•¿åº¦åˆç†
            return 2 <= len(value) <= 50
        elif field == 'store_name':
            # åº—é“ºåé•¿åº¦åˆç†
            return 2 <= len(value) <= 60
        elif field == 'fax':
            # ä¼ çœŸå·ç éªŒè¯
            digit_count = len(re.findall(r'\d', value))
            return 8 <= digit_count <= 20
        
        return True
    
    def _clean_seller_info_ultimate(self, info):
        """ç»ˆæç‰ˆä¿¡æ¯æ¸…ç†"""
        cleaned = {}
        
        for field, value in info.items():
            if not value:
                cleaned[field] = ''
                continue
            
            # åŸºæœ¬æ¸…ç†
            value = str(value).strip()
            value = re.sub(r'\s+', ' ', value)  # åˆå¹¶ç©ºæ ¼
            value = re.sub(r'^[ï¼š:\-\s=]+', '', value)  # ç§»é™¤å¼€å¤´åˆ†éš”ç¬¦
            value = re.sub(r'[ï¼š:\-\s=]+$', '', value)  # ç§»é™¤ç»“å°¾åˆ†éš”ç¬¦
            
            # å­—æ®µç‰¹å®šæ¸…ç†
            if field == 'phone' or field == 'fax':
                # ä¿ç•™æ•°å­—ã€åŠ å·ã€å‡å·ã€æ‹¬å·ã€ç©ºæ ¼
                value = re.sub(r'[^\d\+\-\(\)\s]', '', value)
                value = re.sub(r'\s+', '', value)  # ç§»é™¤ç©ºæ ¼ä½¿æ ¼å¼ç»Ÿä¸€
            elif field == 'email':
                # é‚®ç®±æ¸…ç†
                value = re.sub(r'\s+', '', value)  # ç§»é™¤æ‰€æœ‰ç©ºæ ¼
            elif field == 'address':
                # åœ°å€æ¸…ç†æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
                value = re.sub(r'\n+', ' ', value)
                value = re.sub(r'\s{2,}', ' ', value)
            
            # é™åˆ¶é•¿åº¦
            if field == 'address':
                cleaned[field] = value[:200]
            elif field == 'business_name':
                cleaned[field] = value[:100]
            else:
                cleaned[field] = value[:80]
        
        return cleaned
    
    def _get_column_mappings(self):
        """è·å–ä¸­æ–‡åˆ—åæ˜ å°„"""
        # äº§å“ä¿¡æ¯åˆ—åä¸­æ–‡æ˜ å°„
        product_columns_mapping = {
            'asin': 'ASINç¼–å·',
            'title': 'äº§å“æ ‡é¢˜',
            'price': 'ä»·æ ¼',
            'rating': 'è¯„åˆ†',
            'review_count': 'è¯„ä»·æ•°é‡',
            'image_url': 'å›¾ç‰‡é“¾æ¥',
            'url': 'äº§å“é“¾æ¥',
            'extracted_at': 'æå–æ—¶é—´'
        }
        
        # å–å®¶ä¿¡æ¯åˆ—åä¸­æ–‡æ˜ å°„
        seller_columns_mapping = {
            'seller_name': 'å–å®¶åç§°',
            'seller_url': 'å–å®¶é“¾æ¥',
            'business_name': 'å…¬å¸åç§°',
            'phone': 'ç”µè¯å·ç ',
            'address': 'åœ°å€',
            'representative': 'ä»£è¡¨äººå§“å',
            'store_name': 'åº—é“ºåç§°',
            'email': 'ç”µå­é‚®ç®±',
            'fax': 'ä¼ çœŸå·ç ',
            'product_title': 'å…³è”äº§å“æ ‡é¢˜',
            'product_url': 'å…³è”äº§å“é“¾æ¥',
            'product_asin': 'å…³è”äº§å“ASIN',
            'extracted_at': 'æå–æ—¶é—´'
        }
        
        return product_columns_mapping, seller_columns_mapping
    
    def _save_data_realtime(self, products, sellers, save_callback=None):
        """å®æ—¶ä¿å­˜æ•°æ®"""
        try:
            if not products:
                return
            
            # åˆ›å»ºDataFrameå¹¶é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            products_df = pd.DataFrame(products)
            sellers_df = pd.DataFrame(sellers) if sellers else pd.DataFrame()
            
            # è·å–ä¸­æ–‡åˆ—åæ˜ å°„
            product_columns_mapping, seller_columns_mapping = self._get_column_mappings()
            
            # é‡å‘½ååˆ—å
            if not products_df.empty:
                products_df = products_df.rename(columns=product_columns_mapping)
            
            if not sellers_df.empty:
                sellers_df = sellers_df.rename(columns=seller_columns_mapping)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(self.current_save_file, engine='openpyxl') as writer:
                products_df.to_excel(writer, sheet_name='äº§å“ä¿¡æ¯', index=False)
                if not sellers_df.empty:
                    sellers_df.to_excel(writer, sheet_name='å–å®¶ä¿¡æ¯', index=False)
            
            if save_callback:
                save_callback(f"ğŸ’¾ å·²ä¿å­˜ {len(products)} ä¸ªäº§å“ï¼Œ{len(sellers)} ä¸ªå–å®¶ä¿¡æ¯åˆ° {self.current_save_file}")
        
        except Exception as e:
            print(f"å®æ—¶ä¿å­˜å¤±è´¥: {e}")
    
    def _save_data_final(self, products, sellers, save_callback=None):
        """æœ€ç»ˆä¿å­˜æ•°æ®"""
        try:
            if not products:
                return
            
            # åˆ›å»ºDataFrameå¹¶é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            products_df = pd.DataFrame(products)
            sellers_df = pd.DataFrame(sellers) if sellers else pd.DataFrame()
            
            # è·å–ä¸­æ–‡åˆ—åæ˜ å°„
            product_columns_mapping, seller_columns_mapping = self._get_column_mappings()
            
            # é‡å‘½ååˆ—å
            if not products_df.empty:
                products_df = products_df.rename(columns=product_columns_mapping)
            
            if not sellers_df.empty:
                sellers_df = sellers_df.rename(columns=seller_columns_mapping)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(self.current_save_file, engine='openpyxl') as writer:
                products_df.to_excel(writer, sheet_name='äº§å“ä¿¡æ¯', index=False)
                if not sellers_df.empty:
                    sellers_df.to_excel(writer, sheet_name='å–å®¶ä¿¡æ¯', index=False)
            
            # åŒæ—¶ä¿å­˜CSVæ ¼å¼ï¼ˆä¹Ÿä½¿ç”¨ä¸­æ–‡åˆ—åï¼‰
            csv_file = self.current_save_file.replace('.xlsx', '_products.csv')
            products_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            if not sellers_df.empty:
                sellers_csv = self.current_save_file.replace('.xlsx', '_sellers.csv')
                sellers_df.to_csv(sellers_csv, index=False, encoding='utf-8-sig')
            
            if save_callback:
                save_callback(f"âœ… æœ€ç»ˆä¿å­˜å®Œæˆï¼äº§å“: {len(products)}ï¼Œå–å®¶: {len(sellers)}")
                save_callback(f"ğŸ“ æ–‡ä»¶ä½ç½®: {self.current_save_file}")
        
        except Exception as e:
            print(f"æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
    
    def stop_search(self):
        """åœæ­¢æœç´¢"""
        self.is_searching = False


class UltimateScraperGUI:
    """ç»ˆæç‰ˆGUIç•Œé¢"""
    
    def __init__(self, root):
        self.root = root
        self.root.title("Amazon Japan å–å®¶ä¿¡æ¯æå–å·¥å…· - ç»ˆæç‰ˆ v4.0")
        self.root.geometry("900x700")
        self.root.configure(bg='#f0f0f0')
        
        # è®¾ç½®æ ·å¼
        self.style = ttk.Style()
        self.style.theme_use('clam')
        
        self.scraper = UltimateAmazonScraper()
        self.search_thread = None
        self.is_searching = False
        
        self.setup_gui()
    
    def setup_gui(self):
        """è®¾ç½®GUIç•Œé¢"""
        # ä¸»æ¡†æ¶
        main_frame = ttk.Frame(self.root, padding="20")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text="ğŸš€ Amazon Japan ç»ˆæç‰ˆå–å®¶ä¿¡æ¯æå–å·¥å…·", 
                               font=('Arial', 16, 'bold'))
        title_label.grid(row=0, column=0, columnspan=2, pady=(0, 20))
        
        # æœç´¢é…ç½®åŒºåŸŸ
        search_frame = ttk.LabelFrame(main_frame, text="ğŸ” æœç´¢é…ç½®", padding="15")
        search_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 15))
        
        # å…³é”®è¯è¾“å…¥
        ttk.Label(search_frame, text="æœç´¢å…³é”®è¯:", font=('Arial', 10, 'bold')).grid(row=0, column=0, sticky=tk.W, pady=5)
        self.keyword_var = tk.StringVar()
        keyword_entry = ttk.Entry(search_frame, textvariable=self.keyword_var, width=40, font=('Arial', 10))
        keyword_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(10, 0), pady=5)
        
        # æç¤ºæ–‡æœ¬
        tip_label = ttk.Label(search_frame, text="ğŸ’¡ æ”¯æŒä»»ä½•å•†å“å…³é”®è¯ï¼Œå¦‚ï¼šæ‰‹æœºå£³ã€æ•°æ®çº¿ã€å°å•†å“ç­‰", 
                             font=('Arial', 9), foreground='#666')
        tip_label.grid(row=1, column=0, columnspan=2, sticky=tk.W, pady=(5, 10))
        
        # æ— é™æœç´¢è¯´æ˜
        unlimited_label = ttk.Label(search_frame, text="ğŸ”„ æ— é™åˆ¶æœç´¢æ¨¡å¼ï¼šæƒ³æœå¤šä¹…æœå¤šä¹…ï¼Œå®æ—¶ä¿å­˜æ•°æ®", 
                                   font=('Arial', 10, 'bold'), foreground='#0066cc')
        unlimited_label.grid(row=2, column=0, columnspan=2, sticky=tk.W, pady=(0, 10))
        
        # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
        control_frame = ttk.Frame(main_frame)
        control_frame.grid(row=2, column=0, columnspan=2, pady=(0, 15))
        
        self.start_button = ttk.Button(control_frame, text="ğŸš€ å¼€å§‹æ— é™æœç´¢", 
                                      command=self.start_unlimited_search, 
                                      style='Accent.TButton')
        self.start_button.grid(row=0, column=0, padx=(0, 10))
        
        self.stop_button = ttk.Button(control_frame, text="â¹ï¸ åœæ­¢æœç´¢", 
                                     command=self.stop_search, 
                                     state='disabled')
        self.stop_button.grid(row=0, column=1, padx=(0, 10))
        
        self.open_folder_button = ttk.Button(control_frame, text="ğŸ“ æ‰“å¼€ä¿å­˜æ–‡ä»¶å¤¹", 
                                           command=self.open_save_folder)
        self.open_folder_button.grid(row=0, column=2)
        
        # è¿›åº¦æ˜¾ç¤ºåŒºåŸŸ
        progress_frame = ttk.LabelFrame(main_frame, text="ğŸ“Š æœç´¢è¿›åº¦", padding="15")
        progress_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 15))
        
        # è¿›åº¦æ¡
        self.progress_var = tk.StringVar(value="å‡†å¤‡å¼€å§‹æœç´¢...")
        progress_label = ttk.Label(progress_frame, textvariable=self.progress_var, font=('Arial', 10))
        progress_label.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats_frame = ttk.Frame(progress_frame)
        stats_frame.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        self.products_count_var = tk.StringVar(value="äº§å“æ•°é‡: 0")
        self.sellers_count_var = tk.StringVar(value="å–å®¶æ•°é‡: 0")
        self.time_elapsed_var = tk.StringVar(value="è¿è¡Œæ—¶é—´: 00:00:00")
        
        ttk.Label(stats_frame, textvariable=self.products_count_var, font=('Arial', 9)).grid(row=0, column=0, padx=(0, 20))
        ttk.Label(stats_frame, textvariable=self.sellers_count_var, font=('Arial', 9)).grid(row=0, column=1, padx=(0, 20))
        ttk.Label(stats_frame, textvariable=self.time_elapsed_var, font=('Arial', 9)).grid(row=0, column=2)
        
        # æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
        log_frame = ttk.LabelFrame(main_frame, text="ğŸ“ æœç´¢æ—¥å¿—", padding="15")
        log_frame.grid(row=4, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 15))
        
        # åˆ›å»ºæ–‡æœ¬æ¡†å’Œæ»šåŠ¨æ¡
        self.log_text = tk.Text(log_frame, height=15, width=80, font=('Consolas', 9))
        scrollbar = ttk.Scrollbar(log_frame, orient="vertical", command=self.log_text.yview)
        self.log_text.configure(yscrollcommand=scrollbar.set)
        
        self.log_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        
        # é…ç½®ç½‘æ ¼æƒé‡
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=1)
        main_frame.rowconfigure(4, weight=1)
        search_frame.columnconfigure(1, weight=1)
        progress_frame.columnconfigure(0, weight=1)
        log_frame.columnconfigure(0, weight=1)
        log_frame.rowconfigure(0, weight=1)
        
        # åˆå§‹åŒ–æ—¶é—´
        self.start_time = None
        self.update_timer()
    
    def start_unlimited_search(self):
        """å¼€å§‹æ— é™åˆ¶æœç´¢"""
        keyword = self.keyword_var.get().strip()
        if not keyword:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥æœç´¢å…³é”®è¯")
            return
        
        if self.is_searching:
            messagebox.showwarning("è­¦å‘Š", "æœç´¢æ­£åœ¨è¿›è¡Œä¸­")
            return
        
        # æ›´æ–°UIçŠ¶æ€
        self.is_searching = True
        self.start_button.config(state='disabled')
        self.stop_button.config(state='normal')
        
        # æ¸…ç©ºæ—¥å¿—
        self.log_text.delete(1.0, tk.END)
        
        # é‡ç½®ç»Ÿè®¡
        self.products_count_var.set("äº§å“æ•°é‡: 0")
        self.sellers_count_var.set("å–å®¶æ•°é‡: 0")
        self.start_time = time.time()
        
        # å¯åŠ¨æœç´¢çº¿ç¨‹
        self.search_thread = threading.Thread(
            target=self.search_worker,
            args=(keyword,),
            daemon=True
        )
        self.search_thread.start()
        
        self.log_message(f"ğŸš€ å¼€å§‹æ— é™åˆ¶æœç´¢: {keyword}")
        self.log_message("ğŸ’¡ å¯ä»¥æœ€å°åŒ–çª—å£ï¼Œæœç´¢å°†åœ¨åå°ç»§ç»­è¿è¡Œ")
        self.log_message("ğŸ’¾ æ•°æ®å°†è‡ªåŠ¨ä¿å­˜ï¼Œæ— éœ€æ‹…å¿ƒä¸¢å¤±")
    
    def search_worker(self, keyword):
        """æœç´¢å·¥ä½œçº¿ç¨‹"""
        try:
            self.scraper.unlimited_search(
                keyword=keyword,
                progress_callback=self.update_progress,
                stop_flag=lambda: self.is_searching and self.scraper.is_searching,
                save_callback=self.log_message
            )
        except Exception as e:
            self.log_message(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            # æ¢å¤UIçŠ¶æ€
            self.root.after(0, self.search_completed)
    
    def stop_search(self):
        """åœæ­¢æœç´¢"""
        if not self.is_searching:
            return
        
        self.is_searching = False
        self.scraper.stop_search()
        self.log_message("â¹ï¸ æ­£åœ¨åœæ­¢æœç´¢...")
    
    def search_completed(self):
        """æœç´¢å®Œæˆåçš„UIæ›´æ–°"""
        self.is_searching = False
        self.start_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.log_message("âœ… æœç´¢å·²å®Œæˆæˆ–åœæ­¢")
    
    def update_progress(self, message):
        """æ›´æ–°è¿›åº¦æ˜¾ç¤º"""
        def update_ui():
            self.progress_var.set(message)
            self.log_message(message)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.products_count_var.set(f"äº§å“æ•°é‡: {self.scraper.total_products_found}")
            self.sellers_count_var.set(f"å–å®¶æ•°é‡: {self.scraper.total_sellers_extracted}")
        
        self.root.after(0, update_ui)
    
    def log_message(self, message):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
        def add_log():
            timestamp = datetime.now().strftime("%H:%M:%S")
            log_entry = f"[{timestamp}] {message}\n"
            self.log_text.insert(tk.END, log_entry)
            self.log_text.see(tk.END)
        
        self.root.after(0, add_log)
    
    def update_timer(self):
        """æ›´æ–°è¿è¡Œæ—¶é—´"""
        if self.start_time and self.is_searching:
            elapsed = time.time() - self.start_time
            hours = int(elapsed // 3600)
            minutes = int((elapsed % 3600) // 60)
            seconds = int(elapsed % 60)
            self.time_elapsed_var.set(f"è¿è¡Œæ—¶é—´: {hours:02d}:{minutes:02d}:{seconds:02d}")
        
        # æ¯ç§’æ›´æ–°ä¸€æ¬¡
        self.root.after(1000, self.update_timer)
    
    def open_save_folder(self):
        """æ‰“å¼€ä¿å­˜æ–‡ä»¶å¤¹"""
        import subprocess
        import platform
        
        save_path = os.path.abspath(self.scraper.save_directory)
        
        try:
            if platform.system() == "Windows":
                os.startfile(save_path)
            elif platform.system() == "Darwin":  # macOS
                subprocess.run(["open", save_path])
            else:  # Linux
                subprocess.run(["xdg-open", save_path])
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"æ— æ³•æ‰“å¼€æ–‡ä»¶å¤¹: {e}")


def main():
    """ä¸»å‡½æ•°"""
    root = tk.Tk()
    app = UltimateScraperGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()
